[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7295229.svg)](https://doi.org/10.5281/zenodo.7295229)



# Euclid: Modelling massive neutrinos in cosmology — a code comparison (data repository)
This repository contains the main data processing pipeline used to make all
figures included in the paper
‘Euclid: Modelling massive neutrinos in cosmology — a code comparison’.

- [Euclid: Modelling massive neutrinos in cosmology — a code comparison (data repository)](#euclid--modelling-massive-neutrinos-in-cosmology---a-code-comparison--data-repository)
  * [Introduction](#introduction)
  * [Figures](#figures)
  * [Power spectra](#power-spectra)
  * [Demonstrations](#demonstrations)
    + [Cosmology](#cosmology)
    + [Initial conditions](#initial-conditions)
  * [Data](#data)
    + [Data directory structure](#data-directory-structure)
    + [Figure data](#figure-data)
    + [Snapshots](#snapshots)
    + [Initial conditions](#initial-conditions-1)
      - [Primordial phases](#primordial-phases)
  * [Cleanup](#cleanup)
  * [Required libraries and tools](#required-libraries-and-tools)
  * [Docker](#docker)
  * [Keeping the Git repository sane](#keeping-the-git-repository-sane)



## Introduction
All figures can be found pre-generated in the `figure` directory. To
regenerate them from data, do
```bash
make clean-figure && python=/path/to/python make
```
with `/path/to/python` the path to the Python 3 interpreter to use (see
[Required libraries and tools](#required-libraries-and-tools) for required
Python packages). The necessary data will be downloaded from
[Zenodo](https://zenodo.org/), amounting to a 6 GB download that takes up
18 GB of disk space once extracted. See the [Figures](#figures) section for
further details.

For computing power spectra from snapshots,
see [Power spectra](#power-spectra).

Demonstrations of how the cosmology is specified and how initial conditions
are set up can be found in the [Demonstrations](#demonstrations) section.

Further data (simulation snapshots, initial conditions) are made available,
see the [Data](#data) section.



## Figures
The figures (already present in the `figure` directory) can be regenerated by
running
```bash
python=/path/to/python make figure
```
Missing data will be downloaded as needed (see [Figure data](#figure-data)
for details). Already existing figures will be skipped. To build the figures
in parallel, use e.g.
```bash
python=/path/to/python make -j 4 figure
```
You can remove all figures with
```bash
make clean-figure
```
The Python scripts used for generating the figures are found in the `script`
directory, one script per figure.



## Power spectra
As snapshots are available in the case of GADGET-3, rather than using the
downloaded GADGET-3 power spectrum data files for the figures, these power
spectrum data files can alternatively be computed from the snapshots.

To compute all power spectra from available GADGET-3 snapshots, do
```bash
make python=/path/to/python powerspec
```
As this only does the computation for existing snapshots, you must download
these in advance. If nothing seems to happen when running the above,
try downloading e.g. this snapshot (see [Snapshots](#snapshots) for details):
```bash
make data-snapshot-0.0eV-fiducial-z0
```
(2.5 GB download, 4 GB after extraction). If now building the `powerspec`
Make target still does nothing, it is because you already have the power
spectrum data computed. To remove it, do
```bash
make clean-powerspec
```

Computing the various power spectra requires at most 30 GB of RAM. Part of
these computations can be parallelized by setting `OMP_NUM_THREADS`, e.g.
```bash
export OMP_NUM_THREADS=8
```

Multiple different kinds of power spectra are computed; matter auto power
spectra, neutrino auto power spectra, matter-neutrino auto power spectra,
matter-neutrino cross power spectra, matter-halo cross spectra. For the
matter-halo cross spectra, halo catalogs are needed, which must be obtained as
part of the figure data. To get the halo catalogs and not the pre-computed
power spectra, use
```bash
make data-figure
make clean-powerspec
```

The script employed for computing the power spectra is found within the
`script` directory and is called `compute_powerspec.py`.



## Demonstrations
The `demo` directory contains scripts that are not used directly for the
paper, but are instead included here to demonstrate and document critical
aspects of the computations that goes into it. Two such demonstrations are
included; [Cosmology](#cosmology) which deals with the cosmological
specifications and computation of the linear power spectrum, and
[Initial conditions](#initial-conditions) which shows how to construct a 3D
realization of the matter density field from the
[primordial phases](#primordial-phases) and linear power spectrum. As these
scripts are made for demonstration purposes only, they are written as to be
easily understandable, though at the cost of having suboptimal runtimes.



### Cosmology
The `demo/cosmology.py` script implements the four cosmologies (one for each
neutrino mass) and uses [CLASS](http://class-code.net/) to compute the linear
matter power spectrum, plotting it together with the GADGET-3 results, if
available among the [data](#figure-data). We shall not list the cosmological
parameters here, as these should instead be looked up in `demo/cosmology.py`.
A few remarks is however in order:
* The value of `Omega_cdm` varies with the neutrino mass, with the intention
  of having the cold dark matter and neutrino have a combined density
  parameter of 0.27.
* For massless neutrinos, the CLASS ‘ur’ (ultra-relativistic) species is used,
  with a weight of `N_eff = 3.046`. For massive neutrinos of
  `mass ∈ {0.15, 0.3, 0.6} eV`, this is replaced with a single,
  thrice-degenerate ‘ncdm’ (non-cold dark matter) species of mass `mass/3`
  and a temperature again amounting to having `N_eff = 3.046`.
  The added precision settings used for the massive neutrino cosmologies are
  somewhat arbitrary and does not reflect parameter values used for all
  simulations within this project.

You can run the `demo/cosmology.py` through Python directly, or use
```bash
make python=/path/to/python demo-cosmology
```
which will result in `demo/cosmology.pdf`.



### Initial conditions
The `demo/ic.py` script implements realization of the initial matter density
field, as needed for initial condition generation. A 3D Fourier grid is
populated with random noise of "[fixed](https://arxiv.org/abs/1603.05253)"
amplitudes given by the square root of the power spectrum. The random phases
are read in from an [external file](#primordial-phases), which must then be
applied to the grid in the correct order. For this, a space-filling curve is
used, see the `get_curve_key()` function for details. The grid is then Fourier
transformed to real space, obtaining a realization of the linear matter
density contrast field.

To show that the amplitudes of the realization are correct, its power spectrum
is computed and plotted alongside that of the linear theory input power
spectrum as well as that of the initial condition snapshot, if available among
the [data](#initial-conditios-1). To show that the phases of the realization
matches those of the initial condition snapshot, the matter particles of the
initial condition snapshot are interpolated onto a grid, which is then
deconvolved. A 2D projection of a slice of both the realized grid and the grid
obtained from the initial condition snapshot is then plotted,
which should look extremely similar.

You can run the `demo/ic.py` through Python directly, or use
```bash
make python=/path/to/python demo-ic
```
which will result in `demo/ic.pdf`.



## Data

### Data directory structure
All data files will appear in the `data` directory, which have the following
directory structure:
- `data/`
  - `0.0eV/`
    - `ic/`
      - `phase`
      - `snapshot.*`
    - `gadget3/`
      - `z0/`
        - `snapshot/`
          - `snapshot.*`
        - `powerspec_cdm`
        - ⋯  (other data files)
      - `z1/`
        - `snapshot/`
          - `snapshot.*`
        - `powerspec_cdm`
        - ⋯  (other data files)
    - ⋯  (other codes)
  - `0.0eV_HR/`
    - ⋯  (similar to `0.0eV/`)
  - `0.0eV_1024Mpc/`
    - ⋯  (similar to `0.0eV/`)
  - `0.15eV/`
    - ⋯  (similar to `0.0eV/`)
  - `0.15eV_HR/`
    - ⋯  (similar to `0.0eV/`)
  - `0.15eV_1024Mpc/`
    - ⋯  (similar to `0.0eV/`)
  - `0.3eV/`
    - ⋯  (similar to `0.0eV/`)
  - `0.6eV/`
    - ⋯  (similar to `0.0eV/`)

with the first-level directory labelling the simulation by the neutrino mass
and one of ‘fiducial’ (512 Mpc/h box, 512³ particles), ‘HR’ (512 Mpc/h box,
1024³ particles) or ‘1024Mpc’ (1024 Mpc/h box, 1024³ particles). The `ic`
directories contain initial conditions in the form of a snapshot. The special
`phase` file, providing the primordial random phases, is exclusive to the
`0.0eV/ic` directory (all simulations share the same phases). Each snapshot
(be it initial conditions or final results) is distributed across several
files, as indicated by the asterisk in `snapshot.*`. Snapshots are only made
available for GADGET-3, the reference simulation code employed for this
project.

The following subsections describe how to download various parts of the data.



### Figure data
The data necessary to generate the figures can be downloaded via
```bash
make data-figure
```
This data contains various kinds of power spectra, bispectra and halo catalogs
for the different simulations, codes and output redshifts, and takes up 18 GB
of disk space.



### Snapshots
Simulation snapshots are available for GADGET-3 at redshifts 1 and 0. For the
cosmologies with massive neutrinos, the snapshots contain both matter and
neutrino particles.

In total there are 24 simulation snapshots, corresponding to the 12
simulations (see the section on
[Data director structure](#data-directory-structure)) and two output
redshifts. You can download these one at a time, e.g.
```bash
make data-snapshot-0.3eV-fiducial-z0
make data-snapshot-0.0eV-1024Mpc-z0
make data-snapshot-0.15eV-HR-z1
```
All simulation snapshots within a subset can be downloaded as e.g.
```bash
make data-snapshot-0.0eV-fiducial  # all 0.0eV fiducial (z0, z1)
make data-snapshot-1024Mpc-z0      # all 1024Mpc z0 (0.0eV, 0.15eV)
make data-snapshot-HR              # all HR (0.0eV z0, 0.0eV z1, 0.15eV z0, 0.15eV z1)
make data-snapshot                 # all
```
The easiest way to discover all of these is through auto-completion in the
terminal (type `make data-snapshot-` and press tab twice). A single 0.0eV
fiducial snapshot takes up 4 GB of disk space. Having massive neutrinos raises
this by a factor of 2. The 1024Mpc or HR snapshots comes with another factor
of 8.



### Initial conditions
Initial conditions are available for all simulations. These are given as
GADGET snapshots at redshift 127. For the cosmologies with massive neutrinos,
the snapshots contain both matter and neutrino particles.

In total there are 12 initial condition snapshots, corresponding to the 12
simulations (see the section on
[Data director structure](#data-directory-structure)). You can download these
one at a time, e.g.
```bash
make data-ic-0.3eV-fiducial
make data-ic-0.0eV-1024Mpc
make data-ic-0.15eV-HR
```
All initial condition snapshots within a subset can be downloaded as
```bash
make data-ic-fiducial  # all fiducial (0.0eV, 0.15eV, 0.3eV, 0.6eV)
make data-ic-1024Mpc   # all 1024Mpc  (0.0eV, 0.15eV)
make data-ic-HR        # all HR       (0.0eV, 0.15eV)
make data-ic           # all
```
The easiest way to discover all of these is through auto-completion in the
terminal (type `make data-ic-` and press tab twice). A single 0.0eV fiducial
initial condition snapshot takes up 4 GB of disk space. Having massive
neutrinos raises this by a factor of 2. The 1024Mpc or HR initial condition
snapshots comes with another factor of 8.



#### Primordial phases
While the initial condition snapshots are code agnostic in principle, many
cosmological simulation codes require initial conditions in a special format,
e.g. due to the way neutrinos are handled. Besides the specification of the
cosmology and simulation parameters, a set of primordial amplitudes and phases
are further needed to specify the initial conditions. We use
"[fixed](https://arxiv.org/abs/1603.05253)" amplitudes, leaving only the
phases. These phases are available for download using
```bash
make data-phase
```
taking up 2 GB of disk space. All simulations employ these same phases.
For how to use these phases to generate matching initial conditions of
your own, see the [initial conditions demo](#initial-conditions).



## Cleanup
Each `make` ‘build’ target has an associated ‘clean’ target which undoes the
action. The name of the clean target is the same as the build target,
prepended with `clean-`. For example
```bash
make clean-figure       # remove figure PDFs
make clean-powerspec    # remove GADGET-3 power spectra
make clean-demo         # remove demo PDFs
make clean-data-figure  # remove downloaded figure data
# Remove various subsets of the downloaded snapshots
make clean-data-snapshot-0.3eV-fiducial-z0
make clean-data-snapshot-0.0eV-1024Mpc
make clean-data-snapshot-HR
make clean-data-snapshot
```
In addition, some clean targets exists that has no
```bash
make clean       # remove all PDFs and GADGET-3 power spectra
make clean-data  # remove all downloaded data
make distclean   # remove every generated file
```
The easiest way to discover all of these is through auto-completion in the
terminal (type `make clean-` and press tab twice).



## Required libraries and tools
The generation of figures from the various data, as well as the power spectrum
computations from the snapshots, requires the following libraries and tools to
be installed on the system. All should be to be on the `PATH`.
- Python 3.9.12
  - NumPy 1.21.5
  - SciPy 1.7.3
  - Matplotlib 3.3.4
  - [CLASS 2.7.2](https://github.com/lesgourg/class_public/tree/v2.7.2)
  - [Pylians3 729d74c8af324a77a02926c82b89f678856bfdfe](https://github.com/franciscovillaescusa/Pylians3/tree/729d74c8af324a77a02926c82b89f678856bfdfe)
- LaTeX, e.g. TeX Live 2020.20210202-3
- (GNU) Make 4.2.1
- (GNU) wget 1.21.3
- (GNU) tar 1.30
- XZ Utils 5.2.4

The listed version numbers are known to work and coincide with those shipping
with the [published Docker image](#docker), though many other versions will
work as well.



## Docker
All of the [above dependencies](#required-libraries-and-tools) has been
bundled into a custom Docker image, available at
[jmddk/nucodecomp](https://hub.docker.com/r/jmddk/nucodecomp). To run the data
analysis pipeline through Docker, you can e.g. run Docker interactively,
```bash
docker run --rm -it -v ${PWD}:/mnt jmddk/nucodecomp
```
after which all of the above `make` commands are available. You can copy
results produced within the running Docker container (say the `figure`
directory) to your host filesystem using
```bash
cp -r figure /mnt/
```
The `Dockerfile` used to build the Docker image is provided as part of
this repository.



## Keeping the Git repository sane
If you clone this repository and regenerate the figures in either the `figure`
or `demo` directory, Git will detect and report changes to the PDF files due
to differences in the metadata within the files. We thus do not want changes to
the PDFs to count, but as they are committed, adding them to `.gitignore` does
not help. Instead, do the following:
```bash
git update-index --assume-unchanged figure/*.pdf demo/*.pdf
```

